{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_13664\\843051816.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, 'html.parser').get_text()\n",
      "C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_13664\\843051816.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, 'html.parser').get_text()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # HTML 태그 제거\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # 정규표현식을 사용하여 한글만 남기고 나머지는 제거\n",
    "    text = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", text)\n",
    "\n",
    "    # 형태소 분석\n",
    "    tokens = okt.morphs(text, stem=True)\n",
    "\n",
    "    # 불용어 제거\n",
    "    with open('data/한글불용어100.txt') as st:\n",
    "        lines = st.readlines()\n",
    "    \n",
    "    stop_words = []\n",
    "    for line in lines:\n",
    "        word = line.split('\\t')[0]\n",
    "        stop_words.append(word)\n",
    "        stop_words = [line.split('\\t')[0] for line in lines]\n",
    "    \n",
    "    \n",
    "    # 형태소 분석기를 돌려서 나온 결과에서 불용어 제거하기\n",
    "    morphs = okt.morphs(text, stem=True)\n",
    "    clean_morphs = [morph for morph in morphs if morph not in stop_words]\n",
    "    clean_morphs\n",
    "    \n",
    "    # Feature 변환을 위한 Vectorizer의 입력은 리스트가 아닌 문자열이어야 함\n",
    "    clean_text = ' '.join(clean_morphs)\n",
    "    \n",
    "    # 정제된 텍스트 반환\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def process_qa_csv(input_file, output_file):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(input_file, encoding='utf-8')\n",
    "\n",
    "    # '질문'과 '답변' 열에 대해 정제 함수 적용\n",
    "    df['정제된_질문'] = df['질문'].apply(preprocess_text)\n",
    "    df['정제된_답변'] = df['답변'].apply(preprocess_text)\n",
    "\n",
    "    # 질문과 답변을 쌍으로 가지는 형태로 바꾸기\n",
    "    chatbot_data = [{'질문': q, '답변': a} for q, a in zip(df['정제된_질문'], df['정제된_답변'])]\n",
    "\n",
    "    # 데이터프레임으로 변환\n",
    "    chatbot_df = pd.DataFrame(chatbot_data)\n",
    "\n",
    "    # 정제된 데이터를 CSV 파일로 저장\n",
    "    chatbot_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "# 사용 예시\n",
    "process_qa_csv('data/팁308 copy.csv', 'data/chatbot_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하다 유창 효 공인 중개사 이다 부동산 매매 시 유의 하다 점 은 부동산 을 매매 하다 매도 자 가 실제 권한 이 있다 소유자 인지 그리고 당사자 인지 에 대한 확인 이 중요하다 사 려고 하 는 매물 에 현황 을 정확하다 파악 하다 하다 쉽다 예 로 건축물 대장 토지대장 등기부 등본 을 통해 불법 건축물 여부 면적 토지 점유 하다 권리 근저당 등 을 전부 확인 하다 하다 마지막 으로 분쟁 의 소지 가 예상 되다 부분 은 특약 으로 기재 하 되다 반드시 명확하다 작성 하다 위반 시 계약해지 나 손해배상 등 을 명시 하다 좋다\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "def load_data(csv_path):\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    return df\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text, tokenizer):\n",
    "    tokens = tokenizer.morphs(text)  # 형태소 분석\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 챗봇 함수 구현\n",
    "def create_chatbot(csv_path, tokenizer):\n",
    "    # 데이터 불러오기\n",
    "    df = load_data(csv_path)\n",
    "\n",
    "    # 텍스트 전처리\n",
    "    df['processed_question'] = df['질문'].apply(lambda x: preprocess_text(x, tokenizer))\n",
    "\n",
    "    # TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['processed_question'])\n",
    "\n",
    "    # 챗봇 함수\n",
    "    def chatbot(user_input):\n",
    "        # 사용자 입력 전처리\n",
    "        processed_input = preprocess_text(user_input, tokenizer)\n",
    "\n",
    "        # 사용자 입력을 TF-IDF로 변환\n",
    "        input_vector = vectorizer.transform([processed_input])\n",
    "\n",
    "        # 코사인 유사도 계산\n",
    "        similarities = cosine_similarity(input_vector, tfidf_matrix).flatten()\n",
    "\n",
    "        # 가장 유사한 질문의 인덱스 찾기\n",
    "        max_similarity_index = similarities.argmax()\n",
    "\n",
    "        # 해당 인덱스에 대응하는 답변 출력\n",
    "        response = df.loc[max_similarity_index, '답변']\n",
    "        return response\n",
    "\n",
    "    return chatbot\n",
    "\n",
    "# 챗봇 생성\n",
    "csv_path = 'data/chatbot_data.csv'\n",
    "tokenizer = Okt()  # 형태소 분석기에 맞게 수정\n",
    "chatbot = create_chatbot(csv_path, tokenizer)\n",
    "\n",
    "# 테스트\n",
    "user_input = \"월세 주의점.\"\n",
    "response = chatbot(user_input)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
